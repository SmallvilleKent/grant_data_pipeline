# ğŸ“ Grant Extractor Pipeline

This is a lightweight, cost-efficient AI pipeline to **automatically extract structured grant information from web pages and PDF documents, and store it into a Google Sheets spreadsheet.**

It uses:
- âœ… **Python** for scraping, PDF parsing, and orchestration
- âœ… **OpenAI GPT-3.5-turbo** for structured data extraction
- âœ… **Google Sheets API** to save results
- âœ… **GitHub Actions** to run daily, totally serverless and free

---

## ğŸš€ Features
- ğŸ” Scrapes grant data from both **HTML web pages** and **PDF files**
- ğŸ¤– Uses an LLM to extract key fields like funder name, amount, deadline, eligibility, and link
- ğŸ“ Appends new data to a Google Sheet
- â° Runs automatically on a schedule via GitHub Actions (daily at 8 AM UTC)
- ğŸ’° Costs near $0/month â€” only minor OpenAI charges for API calls

---

## âš™ï¸ How it works
```
[ GitHub Actions ]
       â¬‡
[ Python script ]
       â¬‡
[ Scrapes HTML & PDFs ]
       â¬‡
[ GPT-3.5-turbo extracts JSON ]
       â¬‡
[ Appends to Google Sheets ]
```

---

## ğŸ“‚ Project structure
```
grant_extractor/
â”‚
â”œâ”€â”€ run_pipeline.py       # Main pipeline script
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ .env                  # Local testing config (not committed)
â”‚
â””â”€â”€ .github/
    â””â”€â”€ workflows/
        â””â”€â”€ workflow.yml  # GitHub Actions CI/CD scheduler
```

---

## ğŸš€ Getting started

### âœ… 1. Clone the repository
```bash
git clone https://github.com/your-username/grant_extractor.git
cd grant_extractor
```

### âœ… 2. Set up local environment
Install dependencies:
```bash
pip install -r requirements.txt
```

Create a `.env` file (for local testing only):
```
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxx
GOOGLE_CREDS_JSON={"type":"service_account",...}
```
- `GOOGLE_CREDS_JSON` should be the **full service account JSON** on one line.

---

## ğŸƒ Running locally
```bash
python run_pipeline.py
```

---

## ğŸš€ Automated runs via GitHub Actions
This repo uses GitHub Actions to:
- Run the pipeline **every day at 8 AM UTC**
- You can also trigger manually under the â€œActionsâ€ tab.

### ğŸ” Set up repository secrets
Go to your GitHub repo:
- Settings â†’ Secrets â†’ Actions â†’ **New repository secret**
    - `OPENAI_API_KEY`: your OpenAI API key
    - `GOOGLE_CREDS_JSON`: full service account JSON string (one line, no breaks)

---

## ğŸ” What gets extracted?
For each document (HTML or PDF), the pipeline uses an LLM to extract:
- `funder_name`
- `grant_amount`
- `deadline`
- `eligibility`
- `url`

and appends these as new rows in your Google Sheet.

---

## ğŸš€ Deployment summary
âœ… **No servers required.**  
âœ… **Fully automated.**  
âœ… **Costs ~$0.20/month depending on usage.**

